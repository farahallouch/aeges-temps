{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "\n",
    "# plotting params\n",
    "# %matplotlib inline\n",
    "# plt.style.use('seaborn')\n",
    "# plt.rcParams['font.size'] = 10\n",
    "# plt.rcParams['axes.labelsize'] = 10\n",
    "# plt.rcParams['axes.labelweight'] = 'bold'\n",
    "# plt.rcParams['axes.titlesize'] = 10\n",
    "# plt.rcParams['xtick.labelsize'] = 8\n",
    "# plt.rcParams['ytick.labelsize'] = 8\n",
    "# plt.rcParams['legend.fontsize'] = 10\n",
    "# plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "# plt.rcParams['savefig.dpi'] = 100\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# for reloading modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# path params\n",
    "root_dir = '/Users/Farah/Desktop/Matteo/data/'\n",
    "plot_dir = '/Users/Farah/Desktop/Matteo/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each folder is a different date range\n",
    "* Each file in the folder is the same date range but different conditions\n",
    "\n",
    "* Have a graph for same file across 7 samples. This is equivalent to seeing the same condition over 7 different date ranges.\n",
    "* Have a graph for average of v1 and v2 of same file across all 7 samples.\n",
    "\n",
    "### Todo\n",
    "\n",
    "* Fix x-axis\n",
    "* PDF of all\n",
    "* Outliers??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros(s):\n",
    "    if len(s) == 1:\n",
    "        return s.zfill(2)\n",
    "    return s\n",
    "\n",
    "def pad_year(s):\n",
    "    if s[2][0] != '2':\n",
    "        s[2] = '20' + s[2]\n",
    "    \n",
    "def fix_dates(d):\n",
    "    ret = d.split('/')\n",
    "    ret = list(map(pad_zeros, ret))\n",
    "    pad_year(ret)\n",
    "    return \"/\".join(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 46 files.\n"
     ]
    }
   ],
   "source": [
    "# path params\n",
    "data_dir = root_dir + 'sample1/'\n",
    "pattern = '*.csv'\n",
    "\n",
    "# crawl directory and grab filenames\n",
    "names = []\n",
    "for path, subdirs, files in os.walk(data_dir):\n",
    "    for filename in files:\n",
    "        if fnmatch.fnmatch(filename, pattern):\n",
    "            names.append(os.path.join(path, filename))\n",
    "\n",
    "# remove original files\n",
    "names = [x for x in names if 'original' not in x]\n",
    "num_files = len(names)\n",
    "print(\"\\nThere are {} files.\".format(num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_files = [name[48:] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fshl_iw_1.csv',\n",
       " 'Fshl_iw_2.csv',\n",
       " 'Fshl_ow_1.csv',\n",
       " 'Fshl_ow_2.csv',\n",
       " 'Fshm_iw_1.csv',\n",
       " 'Fshm_iw_2.csv',\n",
       " 'Fshm_ow_1.csv',\n",
       " 'Fshm_ow_2.csv',\n",
       " 'Fshs_iw_1.csv',\n",
       " 'Fshs_iw_2.csv',\n",
       " 'Fshs_ow_1.csv',\n",
       " 'Fshs_ow_2.csv',\n",
       " 'Fsul_iw_1.csv',\n",
       " 'Fsul_iw_2.csv',\n",
       " 'Fsul_ow_1.csv',\n",
       " 'Fsum_iw_1.csv',\n",
       " 'Fsum_iw_2.csv',\n",
       " 'Fsum_ow_1.csv',\n",
       " 'Fsum_ow_2.csv',\n",
       " 'Fsus_iw_1.csv',\n",
       " 'Fsus_iw_2.csv',\n",
       " 'Fsus_ow_1.csv',\n",
       " 'Fsus_ow_2.csv',\n",
       " 'Hshl_iw_1.csv',\n",
       " 'Hshl_iw_2.csv',\n",
       " 'Hshl_ow_1.csv',\n",
       " 'Hshl_ow_2.csv',\n",
       " 'Hshm_iw_1.csv',\n",
       " 'Hshm_iw_2.csv',\n",
       " 'Hshm_ow_1.csv',\n",
       " 'Hshm_ow_2.csv',\n",
       " 'Hshs_iw_1.csv',\n",
       " 'Hshs_iw_2.csv',\n",
       " 'Hshs_ow_1.csv',\n",
       " 'Hshs_ow_2.csv',\n",
       " 'resting_1.csv',\n",
       " 'resting_10.csv',\n",
       " 'resting_11.csv',\n",
       " 'resting_2.csv',\n",
       " 'resting_3.csv',\n",
       " 'resting_4.csv',\n",
       " 'resting_5.csv',\n",
       " 'resting_6.csv',\n",
       " 'resting_7.csv',\n",
       " 'resting_8.csv',\n",
       " 'resting_9.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine length of files in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in range(7):\n",
    "    name = root_dir + 'sample{}/'.format(i+1) + unique_files[0]\n",
    "    cols = ['date', 'unit', 'value']\n",
    "    df = pd.read_csv(name, header=None, names=cols)\n",
    "    lengths.append(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1737, 2062, 683, 1571, 2062, 2062, 2062]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Have a graph for same file across 7 samples (Merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = unique_files[1]\n",
    "name = root_dir + 'sample{}/'.format(7) + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(name, header=None, names=cols)\n",
    "idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "df = df.iloc[(idx+1):, :]\n",
    "df = df.reset_index(drop=True)\n",
    "df.value = df.value.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = list(df.date.values)\n",
    "date = list(map(lambda i: i.split(' ')[0], date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beg)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_yr_beg(s):\n",
    "    return s['date'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_yr_end(s):\n",
    "    return s['date'][len(s)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_iw_1.csv\n",
      "Working with Fshl_iw_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexes/api.py:87: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n",
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexes/api.py:57: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  union = _union_indexes(indexes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_ow_1.csv\n",
      "Working with Fshl_ow_2.csv\n",
      "Working with Fshm_iw_1.csv\n",
      "Working with Fshm_iw_2.csv\n",
      "Working with Fshm_ow_1.csv\n",
      "Working with Fshm_ow_2.csv\n",
      "Working with Fshs_iw_1.csv\n",
      "\tFile did not exist in sample2.\n",
      "Working with Fshs_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample7.\n",
      "Working with Fshs_ow_1.csv\n",
      "Working with Fshs_ow_2.csv\n",
      "Working with Fsul_iw_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "Working with Fsul_iw_2.csv\n",
      "\tFile did not exist in sample6.\n",
      "\tFile did not exist in sample7.\n",
      "Working with Fsul_ow_1.csv\n",
      "Working with Fsum_iw_1.csv\n",
      "Working with Fsum_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample6.\n",
      "Working with Fsum_ow_1.csv\n",
      "Working with Fsum_ow_2.csv\n",
      "Working with Fsus_iw_1.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "Working with Fsus_iw_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fsus_ow_1.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "Working with Fsus_ow_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "\tFile did not exist in sample7.\n",
      "Working with Hshl_iw_1.csv\n",
      "Working with Hshl_iw_2.csv\n",
      "\tFile did not exist in sample3.\n",
      "Working with Hshl_ow_1.csv\n",
      "Working with Hshl_ow_2.csv\n",
      "Working with Hshm_iw_1.csv\n",
      "Working with Hshm_iw_2.csv\n",
      "\tFile did not exist in sample3.\n",
      "Working with Hshm_ow_1.csv\n",
      "Working with Hshm_ow_2.csv\n",
      "Working with Hshs_iw_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "Working with Hshs_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample6.\n",
      "Working with Hshs_ow_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "Working with Hshs_ow_2.csv\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "Working with resting_1.csv\n",
      "Working with resting_10.csv\n",
      "Working with resting_11.csv\n",
      "Working with resting_2.csv\n",
      "Working with resting_3.csv\n",
      "Working with resting_4.csv\n",
      "Working with resting_5.csv\n",
      "Working with resting_6.csv\n",
      "Working with resting_7.csv\n",
      "Working with resting_8.csv\n",
      "Working with resting_9.csv\n",
      "\tFile did not exist in sample5.\n"
     ]
    }
   ],
   "source": [
    "def format_date(x, pos=None):\n",
    "    thisind = np.clip(int(x + 0.5), 0, N - 1)\n",
    "    return temp[thisind]\n",
    "\n",
    "error_plots = []\n",
    "\n",
    "for j, file in enumerate(unique_files):\n",
    "    \n",
    "    print(\"Working with {}\".format(file))\n",
    "    names = []\n",
    "    \n",
    "    # create names\n",
    "    for i in range(7):\n",
    "        name = root_dir + 'sample{}/'.format(i+1) + file\n",
    "        names.append(name)\n",
    "    \n",
    "    # create dataframe\n",
    "    data = []\n",
    "    cols = ['date', 'unit', 'value']\n",
    "    dates = []\n",
    "    for i, name in enumerate(names):\n",
    "        try:\n",
    "            # read in csv\n",
    "            df = pd.read_csv(name, header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # grab dates\n",
    "            date = list(df.date.values)\n",
    "            date = list(map(lambda i: i.split(' ')[0], date))\n",
    "            dates.append(date)\n",
    "            # store\n",
    "            data.append(df)\n",
    "        except:\n",
    "            print(\"\\tFile did not exist in sample{}.\".format(i+1))\n",
    "            error_plots.append(file)\n",
    "            zeros = pd.DataFrame(np.zeros((lengths[i], 3)))\n",
    "            data.append(zeros)\n",
    "            date = ['01/01/2999'] * lengths[i]\n",
    "            dates.append(date)\n",
    "            \n",
    "\n",
    "    # concatenate into 1 big dataframe\n",
    "    df = pd.concat(data)\n",
    "    \n",
    "    # create directory\n",
    "    os_path = plot_dir + os.path.splitext(file)[0] + '/'\n",
    "    if not os.path.exists(os_path):\n",
    "        os.makedirs(os_path)\n",
    "        \n",
    "    # flatten dates\n",
    "    dates = [item for sublist in dates for item in sublist]\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    # x-axis\n",
    "    temp = list(map(fix_dates, dates))\n",
    "    N = len(temp)\n",
    "    ind = np.arange(N)\n",
    "    # y-axis\n",
    "    y_vals = df.value\n",
    "    # meta\n",
    "    lbl = os.path.splitext(file)[0]\n",
    "    ax.plot(ind, y_vals, label=lbl, alpha=0.9, linewidth=0.9, color='tab:red')\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
    "    plt.legend(loc='upper right')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Temperature [C]')\n",
    "    ax.set_title(file)\n",
    "    plt.grid(True)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.savefig(os_path + '{}.eps'.format(lbl), format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a graph for same file across 7 samples (Separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_iw_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_iw_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 4\n",
      "plot error 5\n",
      "plot error 6\n",
      "Working with Fshl_ow_1.csv\n",
      "Working with Fshl_ow_2.csv\n",
      "Working with Fshm_iw_1.csv\n",
      "Working with Fshm_iw_2.csv\n",
      "Working with Fshm_ow_1.csv\n",
      "Working with Fshm_ow_2.csv\n",
      "Working with Fshs_iw_1.csv\n",
      "\tFile did not exist in sample2.\n",
      "plot error 2\n",
      "Working with Fshs_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample7.\n",
      "plot error 2\n",
      "plot error 7\n",
      "Working with Fshs_ow_1.csv\n",
      "Working with Fshs_ow_2.csv\n",
      "Working with Fsul_iw_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "plot error 6\n",
      "Working with Fsul_iw_2.csv\n",
      "\tFile did not exist in sample6.\n",
      "\tFile did not exist in sample7.\n",
      "plot error 6\n",
      "plot error 7\n",
      "Working with Fsul_ow_1.csv\n",
      "Working with Fsum_iw_1.csv\n",
      "Working with Fsum_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 2\n",
      "plot error 6\n",
      "Working with Fsum_ow_1.csv\n",
      "Working with Fsum_ow_2.csv\n",
      "Working with Fsus_iw_1.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 4\n",
      "plot error 5\n",
      "plot error 6\n",
      "Working with Fsus_iw_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 4\n",
      "plot error 5\n",
      "plot error 6\n",
      "Working with Fsus_ow_1.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 4\n",
      "plot error 5\n",
      "plot error 6\n",
      "Working with Fsus_ow_2.csv\n",
      "\tFile did not exist in sample4.\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "\tFile did not exist in sample7.\n",
      "plot error 4\n",
      "plot error 5\n",
      "plot error 6\n",
      "plot error 7\n",
      "Working with Hshl_iw_1.csv\n",
      "Working with Hshl_iw_2.csv\n",
      "\tFile did not exist in sample3.\n",
      "plot error 3\n",
      "Working with Hshl_ow_1.csv\n",
      "Working with Hshl_ow_2.csv\n",
      "Working with Hshm_iw_1.csv\n",
      "Working with Hshm_iw_2.csv\n",
      "\tFile did not exist in sample3.\n",
      "plot error 3\n",
      "Working with Hshm_ow_1.csv\n",
      "Working with Hshm_ow_2.csv\n",
      "Working with Hshs_iw_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "plot error 6\n",
      "Working with Hshs_iw_2.csv\n",
      "\tFile did not exist in sample2.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 2\n",
      "plot error 6\n",
      "Working with Hshs_ow_1.csv\n",
      "\tFile did not exist in sample6.\n",
      "plot error 6\n",
      "Working with Hshs_ow_2.csv\n",
      "\tFile did not exist in sample5.\n",
      "\tFile did not exist in sample6.\n",
      "plot error 5\n",
      "plot error 6\n",
      "Working with resting_1.csv\n",
      "Working with resting_10.csv\n",
      "Working with resting_11.csv\n",
      "Working with resting_2.csv\n",
      "Working with resting_3.csv\n",
      "Working with resting_4.csv\n",
      "Working with resting_5.csv\n",
      "Working with resting_6.csv\n",
      "Working with resting_7.csv\n",
      "Working with resting_8.csv\n",
      "Working with resting_9.csv\n",
      "\tFile did not exist in sample5.\n",
      "plot error 5\n"
     ]
    }
   ],
   "source": [
    "def format_date(x, pos=None):\n",
    "    thisind = np.clip(int(x + 0.5), 0, N - 1)\n",
    "    return temp[thisind]\n",
    "\n",
    "error_plots = []\n",
    "\n",
    "for j, file in enumerate(unique_files):\n",
    "    \n",
    "    print(\"Working with {}\".format(file))\n",
    "    names = []\n",
    "    \n",
    "    # create names\n",
    "    for i in range(7):\n",
    "        name = root_dir + 'sample{}/'.format(i+1) + file\n",
    "        names.append(name)\n",
    "    \n",
    "    # create dataframe\n",
    "    data = []\n",
    "    cols = ['date', 'unit', 'value']\n",
    "    dates = []\n",
    "    for i, name in enumerate(names):\n",
    "        try:\n",
    "            # read in csv\n",
    "            df = pd.read_csv(name, header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # grab dates\n",
    "            date = list(df.date.values)\n",
    "            date = list(map(lambda i: i.split(' ')[0], date))\n",
    "            dates.append(date)\n",
    "            # store\n",
    "            data.append(df)\n",
    "        except:\n",
    "            print(\"\\tFile did not exist in sample{}.\".format(i+1))\n",
    "            error_plots.append(file)\n",
    "            date = ['01/01/2999'] * lengths[i]\n",
    "            dates.append(date)\n",
    "            data.append(pd.DataFrame())\n",
    "            \n",
    "    \n",
    "    # create directory\n",
    "    os_path = plot_dir + os.path.splitext(file)[0] + '/'\n",
    "    if not os.path.exists(os_path):\n",
    "        os.makedirs(os_path)\n",
    "            \n",
    "    # plot\n",
    "    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \n",
    "              'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data[i].empty:\n",
    "            print('plot error {}'.format(i+1))\n",
    "            continue\n",
    "        fig, ax = plt.subplots()\n",
    "        # x-axis\n",
    "        temp = list(map(fix_dates, dates[i]))\n",
    "        N = len(temp)\n",
    "        ind = np.arange(N)\n",
    "        # y-axis\n",
    "        y_vals = data[i].value\n",
    "        # meta\n",
    "        lbl = 'sample{}'.format(i+1)\n",
    "        ax.plot(ind, y_vals, label=lbl, alpha=0.9, linewidth=0.9, color=colors[i])\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
    "        plt.legend(loc='upper right')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Temperature [C]')\n",
    "        ax.set_title(file)\n",
    "        plt.grid(True)\n",
    "        fig.autofmt_xdate()\n",
    "        plt.savefig(os_path + 'sample_{}.eps'.format(i+1), format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plots = list(set(error_plots))\n",
    "error_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Average of v1 and v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 35 files.\n"
     ]
    }
   ],
   "source": [
    "# path params\n",
    "data_dir = root_dir + 'sample1/'\n",
    "pattern = '*.csv'\n",
    "\n",
    "# crawl directory and grab filenames\n",
    "names = []\n",
    "for path, subdirs, files in os.walk(data_dir):\n",
    "    for filename in files:\n",
    "        if fnmatch.fnmatch(filename, pattern):\n",
    "            names.append(os.path.join(path, filename))\n",
    "\n",
    "# remove original files\n",
    "names = [x for x in names if 'original' not in x and 'resting' not in x]\n",
    "num_files = len(names)\n",
    "print(\"\\nThere are {} files.\".format(num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Farah/Desktop/Matteo/data/sample1/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_2.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fshl_iw_1.csv',\n",
       " 'Fshl_iw_2.csv',\n",
       " 'Fshl_ow_1.csv',\n",
       " 'Fshl_ow_2.csv',\n",
       " 'Fshm_iw_1.csv',\n",
       " 'Fshm_iw_2.csv',\n",
       " 'Fshm_ow_1.csv',\n",
       " 'Fshm_ow_2.csv',\n",
       " 'Fshs_iw_1.csv',\n",
       " 'Fshs_iw_2.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_files = [name[48:] for name in names]\n",
    "unique_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/data/sample7/Fshs_iw_2.csv']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 8\n",
    "name1 = unique_files[i]\n",
    "name2 = unique_files[i+1]\n",
    "\n",
    "names = []\n",
    "# create names\n",
    "for i in range(7):\n",
    "    n1 = root_dir + 'sample{}/'.format(i+1) + name1\n",
    "    n2 = root_dir + 'sample{}/'.format(i+1) + name2\n",
    "    names.extend([n1, n2])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_both(n1, n2):\n",
    "    cols = ['date', 'unit', 'value']\n",
    "    \n",
    "    # create dataframes\n",
    "    df1 = pd.read_csv(n1, header=None, names=cols)\n",
    "    df2 = pd.read_csv(n2, header=None, names=cols)\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    \n",
    "    # preprocess\n",
    "    idx = df1.index[df1['date'] == 'Date/Time'][0]\n",
    "    df1 = df1.iloc[(idx+1):, :]\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    idx = df2.index[df2['date'] == 'Date/Time'][0]\n",
    "    df2 = df2.iloc[(idx+1):, :]\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df1.value = df1.value.astype(float)\n",
    "    df2.value = df1.value.astype(float)\n",
    "    \n",
    "    # concat\n",
    "    df1.value = pd.concat([df1.value, df2.value], axis=1).mean(axis=1)\n",
    "    \n",
    "    return df1\n",
    "\n",
    "def take_one(n):\n",
    "    cols = ['date', 'unit', 'value']\n",
    "    \n",
    "    # create dataframe\n",
    "    df = pd.read_csv(n, header=None, names=cols)\n",
    "    \n",
    "    # preprocess\n",
    "    idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "    df = df.iloc[(idx+1):, :]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.value = df.value.astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def nothing(i):\n",
    "    zeros = pd.DataFrame(np.zeros((lengths[i], 3)))\n",
    "    date = ['01/01/2999'] * lengths[i]\n",
    "    return zeros, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fshl_iw'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_files[0][:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fshl_iw_1.csv',\n",
       " 'Fshl_iw_2.csv',\n",
       " 'Fshl_ow_1.csv',\n",
       " 'Fshl_ow_2.csv',\n",
       " 'Fshm_iw_1.csv',\n",
       " 'Fshm_iw_2.csv',\n",
       " 'Fshm_ow_1.csv',\n",
       " 'Fshm_ow_2.csv',\n",
       " 'Fshs_iw_1.csv',\n",
       " 'Fshs_iw_2.csv',\n",
       " 'Fshs_ow_1.csv',\n",
       " 'Fshs_ow_2.csv',\n",
       " 'Fsul_iw_1.csv',\n",
       " 'Fsul_iw_2.csv',\n",
       " 'Fsul_ow_1.csv',\n",
       " 'Fsum_iw_1.csv',\n",
       " 'Fsum_iw_2.csv',\n",
       " 'Fsum_ow_1.csv',\n",
       " 'Fsum_ow_2.csv',\n",
       " 'Fsus_iw_1.csv',\n",
       " 'Fsus_iw_2.csv',\n",
       " 'Fsus_ow_1.csv',\n",
       " 'Fsus_ow_2.csv',\n",
       " 'Hshl_iw_1.csv',\n",
       " 'Hshl_iw_2.csv',\n",
       " 'Hshl_ow_1.csv',\n",
       " 'Hshl_ow_2.csv',\n",
       " 'Hshm_iw_1.csv',\n",
       " 'Hshm_iw_2.csv',\n",
       " 'Hshm_ow_1.csv',\n",
       " 'Hshm_ow_2.csv',\n",
       " 'Hshs_iw_1.csv',\n",
       " 'Hshs_iw_2.csv',\n",
       " 'Hshs_ow_1.csv',\n",
       " 'Hshs_ow_2.csv']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 5\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 6\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshl_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fshm_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fshm_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fshs_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tTaking first...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexes/api.py:57: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  union = _union_indexes(indexes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fshs_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fsul_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 7\n",
      "\t\tTaking first...\n",
      "\n",
      "\n",
      "Working with Fsul_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fsum_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fsum_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 5\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 6\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Fsus_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 5\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 6\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexes/api.py:87: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fsus_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 5\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 6\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 7\n",
      "\t\tTaking second...\n",
      "\n",
      "\n",
      "Working with Hshl_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Hshl_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Hshm_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Hshm_ow\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\tTaking first...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n",
      "Working with Hshs_iw\n",
      "\tWorking with sample 1\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 2\n",
      "\t\tTaking second...\n",
      "\tWorking with sample 3\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 4\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 5\n",
      "\t\tAveraging...\n",
      "\tWorking with sample 6\n",
      "\t\t[*] Empty for both!...\n",
      "\tWorking with sample 7\n",
      "\t\tAveraging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d611da355601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mname1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mname2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working with {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(unique_files), 2):\n",
    "    \n",
    "    name1 = unique_files[i]\n",
    "    name2 = unique_files[i+1]\n",
    "    \n",
    "    print('Working with {}'.format(name1[:7]))\n",
    "\n",
    "    names = []\n",
    "    for k in range(7):\n",
    "        n1 = root_dir + 'sample{}/'.format(k+1) + name1\n",
    "        n2 = root_dir + 'sample{}/'.format(k+1) + name2\n",
    "        names.extend([n1, n2])\n",
    "\n",
    "    data = []\n",
    "    dates = []\n",
    "    file = names[0][48:55]\n",
    "\n",
    "    for j in range(0, len(names), 2):\n",
    "\n",
    "        first = names[j]\n",
    "        second = names[j+1]\n",
    "        sample_num = int(first[46])\n",
    "\n",
    "        print('\\tWorking with sample {}'.format(first[46]))\n",
    "\n",
    "        # check if it exists\n",
    "        if os.path.exists(first):\n",
    "            # check second\n",
    "            if os.path.exists(second):\n",
    "                print('\\t\\tAveraging...')\n",
    "                # average both\n",
    "                d = average_both(first, second)\n",
    "                # grab dates\n",
    "                date = list(d.date.values)\n",
    "                date = list(map(lambda p: p.split(' ')[0], date))\n",
    "                # store\n",
    "                dates.append(date)\n",
    "                data.append(d)\n",
    "            else:\n",
    "                # take the first\n",
    "                print('\\t\\tTaking first...')\n",
    "                d = take_one(first)\n",
    "                # grab dates\n",
    "                date = list(d.date.values)\n",
    "                date = list(map(lambda p: p.split(' ')[0], date))\n",
    "                # store\n",
    "                dates.append(date)\n",
    "                data.append(d)\n",
    "        else:\n",
    "            # check if the second exists\n",
    "            if os.path.exists(second):\n",
    "                # take the second\n",
    "                print('\\t\\tTaking second...')\n",
    "                d = take_one(second)\n",
    "                # grab dates\n",
    "                date = list(d.date.values)\n",
    "                date = list(map(lambda p: p.split(' ')[0], date))\n",
    "                # store\n",
    "                dates.append(date)\n",
    "                data.append(d)\n",
    "            else:\n",
    "                # take nothing and return an empty\n",
    "                print('\\t\\t[*] Empty for both!...')\n",
    "                d, date = nothing(sample_num)\n",
    "                dates.append(date)\n",
    "                data.append(d)\n",
    "    \n",
    "    # concatenate into 1 big dataframe\n",
    "    print('\\n')\n",
    "    avg = pd.concat(data)\n",
    "\n",
    "    # flatten dates\n",
    "    dates = [item for sublist in dates for item in sublist]\n",
    "\n",
    "    # plot\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    # x-axis\n",
    "    temp = list(map(fix_dates, dates))\n",
    "    N = len(temp)\n",
    "    ind = np.arange(N)\n",
    "    # y-axis\n",
    "    y_vals = avg.value\n",
    "    # meta\n",
    "    ax.plot(ind, y_vals, label=file, alpha=0.9, linewidth=0.9, color='tab:red')\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
    "    plt.legend(loc='upper right')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Temperature [C]')\n",
    "    ax.set_title(file)\n",
    "    plt.grid(True)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.savefig(plot_dir + 'averages/' + '{}.eps'.format(file), format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexes/api.py:57: RuntimeWarning: '<' not supported between instances of 'int' and 'str', sort order is undefined for incomparable objects\n",
      "  union = _union_indexes(indexes)\n"
     ]
    }
   ],
   "source": [
    "def format_date(x, pos=None):\n",
    "    thisind = np.clip(int(x + 0.5), 0, N - 1)\n",
    "    return temp[thisind]\n",
    "\n",
    "# concatenate into 1 big dataframe\n",
    "avg = pd.concat(data)\n",
    "\n",
    "# flatten dates\n",
    "dates = [item for sublist in dates for item in sublist]\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "# x-axis\n",
    "temp = list(map(fix_dates, dates))\n",
    "N = len(temp)\n",
    "ind = np.arange(N)\n",
    "# y-axis\n",
    "y_vals = avg.value\n",
    "# meta\n",
    "ax.plot(ind, y_vals, label=file, alpha=0.9, linewidth=0.9, color='tab:red')\n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
    "plt.legend(loc='upper right')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Temperature [C]')\n",
    "ax.set_title(file)\n",
    "plt.grid(True)\n",
    "fig.autofmt_xdate()\n",
    "plt.savefig(plot_dir + 'averages/' + '{}.eps'.format(file), format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "unique = unique_files[5]\n",
    "\n",
    "print(\"Working with: {}\".format(unique))\n",
    "\n",
    "across_folders = [x for x in names if unique in x][1:]\n",
    "\n",
    "data = []\n",
    "dates = []\n",
    "prev = 0\n",
    "for i in range(len(across_folders)):\n",
    "\n",
    "    if skip:\n",
    "        skip = False\n",
    "        continue\n",
    "\n",
    "    cur = int(across_folders[i][48:57][-1])\n",
    "    print(\"prev: {}\".format(prev))\n",
    "    print(\"cur: {}\".format(cur))\n",
    "    print(\"\\tWorking with {}\".format(across_folders[i][48:57]))\n",
    "    \n",
    "    # if last file\n",
    "    if i == len(across_folders) - 1:\n",
    "        print(\"\\tLAST FILE: {}\".format(across_folders[i][48:56][-1]))\n",
    "        df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "        idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "        df = df.iloc[(idx+1):, :]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.value = df.value.astype(float)\n",
    "        data.append(df)\n",
    "    else:\n",
    "        # if sample doesn't exist in both\n",
    "        if cur != prev + 1:\n",
    "            print(\"\\t\\tMissing in both: {}\".format(prev+1))\n",
    "            diff = cur - (prev + 1)\n",
    "            for j in range(diff):\n",
    "                data.append(pd.DataFrame())\n",
    "                # date = ['01/01/2999'] * lengths[i]\n",
    "                # dates.append(date)\n",
    "\n",
    "        # if 2 files ==> average\n",
    "        elif across_folders[i][48:55] == across_folders[i+1][48:55]:\n",
    "            print(\"\\t\\tAverage!\")\n",
    "            skip = True\n",
    "            cols = ['date', 'unit', 'value']\n",
    "            df1 = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            df2 = pd.read_csv(across_folders[i+1], header=None, names=cols)\n",
    "            df1 = df1.copy()\n",
    "            df2 = df2.copy()\n",
    "            idx = df1.index[df1['date'] == 'Date/Time'][0]\n",
    "            df1 = df1.iloc[(idx+1):, :]\n",
    "            df1 = df1.reset_index(drop=True)\n",
    "            idx = df2.index[df2['date'] == 'Date/Time'][0]\n",
    "            df2 = df2.iloc[(idx+1):, :]\n",
    "            df2 = df2.reset_index(drop=True)\n",
    "            df1.value = df1.value.astype(float)\n",
    "            df2.value = df1.value.astype(float)\n",
    "            df1.value = pd.concat([df1.value, df2.value], axis=1).mean(axis=1)\n",
    "            data.append(df1)\n",
    "            prev = 0\n",
    "\n",
    "        # else 1 file ==> like before\n",
    "        else:\n",
    "            print(\"\\t\\tTaking from folder: {}\".format(across_folders[i][48:57][-1]))\n",
    "            # read in csv\n",
    "            df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "            # save name\n",
    "            prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 220 files.\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "for i in range(7):\n",
    "    data_dir = root_dir + 'sample{}/'.format(i+1)\n",
    "    pattern = '*.csv'\n",
    "\n",
    "    # crawl directory and grab filenames\n",
    "    for path, subdirs, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if fnmatch.fnmatch(filename, pattern):\n",
    "                names.append(os.path.join(path, filename))\n",
    "                \n",
    "# remove original files\n",
    "names = [x for x in names if 'original' not in x and 'resting' not in x]\n",
    "num_files = len(names)\n",
    "print(\"\\nThere are {} files.\".format(num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique files\n",
    "unique_files = [name[40:47] for name in names]\n",
    "unique_files = list(set(unique_files))\n",
    "len(unique_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Farah/Desktop/Matteo/data/sample1/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsul_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsum_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsus_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsus_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsus_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Fsus_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample1/Hshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsul_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsus_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsus_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsus_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Fsus_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample2/Hshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsul_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsum_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsus_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsus_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsus_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Fsus_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample3/Hshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsul_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsum_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample4/Hshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsul_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsum_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample5/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsh_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsul_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsum_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsus_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsus_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Fsus_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample7/Hshs_ow_2.csv']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_folders = [x for x in names if unique_files[5] in x][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Farah/Desktop/Matteo/data/sample6/Fshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshm_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fshs_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsul_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsul_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Fsum_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshl_ow_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_iw_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_iw_2.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_ow_1.csv',\n",
       " '/Users/Farah/Desktop/Matteo/data/sample6/Hshm_ow_2.csv']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fshl_iw'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_folders[0][48:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: sample6\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshl_iw_1\n",
      "\t\tTaking from folder: 1\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshl_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshm_iw_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshm_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshs_iw_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fshs_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fsul_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fsum_iw_1\n",
      "\t\tTaking from folder: 1\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Fsum_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Hshl_iw_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Hshl_ow_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Hshm_iw_1\n",
      "\t\tAverage!\n",
      "prev: 0\n",
      "cur: 1\n",
      "\tWorking with Hshm_ow_1\n",
      "\t\tAverage!\n"
     ]
    }
   ],
   "source": [
    "skip = False\n",
    "unique = unique_files[5]\n",
    "\n",
    "print(\"Working with: {}\".format(unique))\n",
    "\n",
    "across_folders = [x for x in names if unique in x][1:]\n",
    "\n",
    "data = []\n",
    "dates = []\n",
    "prev = 0\n",
    "for i in range(len(across_folders)):\n",
    "\n",
    "    if skip:\n",
    "        skip = False\n",
    "        continue\n",
    "\n",
    "    cur = int(across_folders[i][48:57][-1])\n",
    "    print(\"prev: {}\".format(prev))\n",
    "    print(\"cur: {}\".format(cur))\n",
    "    print(\"\\tWorking with {}\".format(across_folders[i][48:57]))\n",
    "    \n",
    "    # if last file\n",
    "    if i == len(across_folders) - 1:\n",
    "        print(\"\\tLAST FILE: {}\".format(across_folders[i][48:56][-1]))\n",
    "        df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "        idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "        df = df.iloc[(idx+1):, :]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df.value = df.value.astype(float)\n",
    "        data.append(df)\n",
    "    else:\n",
    "        # if sample doesn't exist in both\n",
    "        if cur != prev + 1:\n",
    "            print(\"\\t\\tMissing in both: {}\".format(prev+1))\n",
    "            diff = cur - (prev + 1)\n",
    "            for j in range(diff):\n",
    "                data.append(pd.DataFrame())\n",
    "                # date = ['01/01/2999'] * lengths[i]\n",
    "                # dates.append(date)\n",
    "\n",
    "        # if 2 files ==> average\n",
    "        elif across_folders[i][48:55] == across_folders[i+1][48:55]:\n",
    "            print(\"\\t\\tAverage!\")\n",
    "            skip = True\n",
    "            cols = ['date', 'unit', 'value']\n",
    "            df1 = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            df2 = pd.read_csv(across_folders[i+1], header=None, names=cols)\n",
    "            df1 = df1.copy()\n",
    "            df2 = df2.copy()\n",
    "            idx = df1.index[df1['date'] == 'Date/Time'][0]\n",
    "            df1 = df1.iloc[(idx+1):, :]\n",
    "            df1 = df1.reset_index(drop=True)\n",
    "            idx = df2.index[df2['date'] == 'Date/Time'][0]\n",
    "            df2 = df2.iloc[(idx+1):, :]\n",
    "            df2 = df2.reset_index(drop=True)\n",
    "            df1.value = df1.value.astype(float)\n",
    "            df2.value = df1.value.astype(float)\n",
    "            df1.value = pd.concat([df1.value, df2.value], axis=1).mean(axis=1)\n",
    "            data.append(df1)\n",
    "            prev = 0\n",
    "\n",
    "        # else 1 file ==> like before\n",
    "        else:\n",
    "            print(\"\\t\\tTaking from folder: {}\".format(across_folders[i][48:57][-1]))\n",
    "            # read in csv\n",
    "            df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "            # save name\n",
    "            prev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 0: sample2\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 1\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "\tWorking with sample2\n",
      "\tMissing in both: 3\n",
      "\tAverage!\n",
      "1 EMPTY!!\n",
      "K: 1\n",
      "cumul: 0\n",
      "34816\n",
      "/Users/Farah/Desktop/Matteo/plots/sample2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "skip = False\n",
    "\n",
    "for j, unique in enumerate(unique_files):\n",
    "    \n",
    "    if j >= 1:\n",
    "        break\n",
    "        \n",
    "    print(\"Working with {}: {}\".format(j, unique))\n",
    "    \n",
    "    across_folders = [x for x in names if unique in x]\n",
    "    \n",
    "    data = []\n",
    "    prev = 0\n",
    "    for i in range(len(across_folders)):\n",
    "    \n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        \n",
    "        cur = int(across_folders[i][32:39][-1])\n",
    "        print(\"\\tWorking with {}\".format(across_folders[i][32:39]))\n",
    "        \n",
    "        # if sample doesn't exist in both\n",
    "        if cur != prev + 1:\n",
    "            print(\"\\tMissing in both: {}\".format(prev+1))\n",
    "            diff = cur - (prev + 1)\n",
    "            for j in range(diff):\n",
    "                data.append(pd.DataFrame())\n",
    "        \n",
    "        # if last file\n",
    "        if i == len(across_folders)-1:\n",
    "            print(\"\\tLAST Taking from folder: {}\".format(across_folders[i][40:49][-1]))\n",
    "            # read in csv\n",
    "            df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "        \n",
    "        # if 2 files ==> average\n",
    "        elif across_folders[i][38] == across_folders[i+1][38]:\n",
    "            print(\"\\tAverage!\")\n",
    "            skip = True\n",
    "            \n",
    "            # create df\n",
    "            cols = ['date', 'unit', 'value']\n",
    "\n",
    "            # read in csv\n",
    "            df1 = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            df2 = pd.read_csv(across_folders[i+1], header=None, names=cols)\n",
    "\n",
    "            df1 = df1.copy()\n",
    "            df2 = df2.copy()\n",
    "\n",
    "            # remove metadata junk\n",
    "            idx = df1.index[df1['date'] == 'Date/Time'][0]\n",
    "            df1 = df1.iloc[(idx+1):, :]\n",
    "            df1 = df1.reset_index(drop=True)\n",
    "\n",
    "            idx = df2.index[df2['date'] == 'Date/Time'][0]\n",
    "            df2 = df2.iloc[(idx+1):, :]\n",
    "            df2 = df2.reset_index(drop=True)\n",
    "\n",
    "            # fix column formats\n",
    "            df1.value = df1.value.astype(float)\n",
    "            df2.value = df1.value.astype(float)\n",
    "\n",
    "            df1.value = pd.concat([df1.value, df2.value], axis=1).mean(axis=1)\n",
    "            data.append(df1)\n",
    "        \n",
    "        # else 1 file ==> like before\n",
    "        else:\n",
    "            print(\"\\tTaking from folder: {}\".format(across_folders[i][40:49][-1]))\n",
    "            # read in csv\n",
    "            df = pd.read_csv(across_folders[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "        \n",
    "        # save name\n",
    "        prev = cur\n",
    "        \n",
    "    # fix emptys   \n",
    "    shit = []\n",
    "    cumul = 0\n",
    "    skipz = 0\n",
    "    for i in range(len(data)):\n",
    "        if skipz > 0:\n",
    "            skipz -= 1\n",
    "            continue\n",
    "        if data[i].empty:\n",
    "            print(\"{} EMPTY!!\".format(i+1))\n",
    "            for k in range(i, len(data)):\n",
    "                if data[k].empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            print(\"K: {}\".format(k))\n",
    "            print(\"cumul: {}\".format(cumul))\n",
    "            # poop = np.arange((i)*len(data[i+1]), (i+1)*len(data[i+1]))\n",
    "            poop = np.arange(cumul, (k+1)*len(data[k]))\n",
    "            y_vals = np.zeros(len(poop))\n",
    "            y_vals[y_vals == 0] = np.NAN\n",
    "            skipz = k - i\n",
    "        else:\n",
    "            y_vals = data[i].value\n",
    "            cumul += len(y_vals)\n",
    "        # poop = np.column_stack([x_vals, y_vals])\n",
    "        shit.append(y_vals)\n",
    "    # print(shit[5][0])\n",
    "    y_s = np.concatenate(shit)\n",
    "    print(len(y_s))\n",
    "    x_s = np.arange(len(y_s))\n",
    "    \n",
    "    # create directory\n",
    "    os_path = plot_dir + os.path.splitext(unique)[0] + '/'\n",
    "    print(os_path)\n",
    "    if not os.path.exists(os_path):\n",
    "        os.makedirs(os_path)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    lbl = os.path.splitext(unique)[0] + '_avg'\n",
    "    plt.plot(x_s, y_s, label=lbl, alpha=0.9, linewidth=0.9)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Temperature [C]')\n",
    "    plt.title(lbl)\n",
    "    plt.savefig(os_path + '{}.eps'.format(lbl), format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = ['Fshl_iw', 'Fshm_iw', 'Fshs_iw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for i in range(7):\n",
    "    data_dir = root_dir + 'sample{}/'.format(i+1)\n",
    "    pattern = '*.csv'\n",
    "\n",
    "    # crawl directory and grab filenames\n",
    "    for path, subdirs, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if fnmatch.fnmatch(filename, pattern):\n",
    "                names.append(os.path.join(path, filename))\n",
    "                \n",
    "# remove original files\n",
    "names = [x for x in names if 'original' not in x and 'resting' not in x]\n",
    "num_files = len(names)\n",
    "print(\"\\nThere are {} files.\".format(num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = []\n",
    "for cb in combs:\n",
    "    sub = [x for x in names if cb in x]\n",
    "    subset.append(sub)\n",
    "subset = [item for sublist in subset for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_folders = [x for x in subset if combs[0] in x]\n",
    "across_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "plots = []\n",
    "for j, unique in enumerate(combs):\n",
    "    print(\"Working with {}: {}\".format(j, unique))\n",
    "        \n",
    "    across_folders = [x for x in subset if unique in x]\n",
    "    \n",
    "    data = []\n",
    "    prev = 0\n",
    "    for i in range(len(across_folders)):\n",
    "\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "\n",
    "        cur = int(across_folders[i][32:39][-1])\n",
    "        print(\"\\tWorking with {}\".format(across_folders[i][32:39]))\n",
    "\n",
    "        if cur != prev + 1:\n",
    "            diff = cur - (prev + 1)\n",
    "            for j in range(diff):\n",
    "                data.append(pd.DataFrame())\n",
    "\n",
    "        # if last file\n",
    "        if i == len(across_folders)-1:\n",
    "            # read in csv\n",
    "            df = pd.read_csv(names[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "\n",
    "        # if 2 files ==> average\n",
    "        elif across_folders[i][38] == across_folders[i+1][38]:\n",
    "            print(\"\\tAverage!\")\n",
    "            skip = True\n",
    "\n",
    "            # create df\n",
    "            cols = ['date', 'unit', 'value']\n",
    "\n",
    "            # read in csv\n",
    "            df1 = pd.read_csv(names[i], header=None, names=cols)\n",
    "            df2 = pd.read_csv(names[i+1], header=None, names=cols)\n",
    "\n",
    "            df1 = df1.copy()\n",
    "            df2 = df2.copy()\n",
    "\n",
    "            # remove metadata junk\n",
    "            idx = df1.index[df1['date'] == 'Date/Time'][0]\n",
    "            df1 = df1.iloc[(idx+1):, :]\n",
    "            df1 = df1.reset_index(drop=True)\n",
    "\n",
    "            idx = df2.index[df2['date'] == 'Date/Time'][0]\n",
    "            df2 = df2.iloc[(idx+1):, :]\n",
    "            df2 = df2.reset_index(drop=True)\n",
    "\n",
    "            # fix column formats\n",
    "            df1.value = df1.value.astype(float)\n",
    "            df2.value = df1.value.astype(float)\n",
    "\n",
    "            df1.value = pd.concat([df1.value, df2.value], axis=1).mean(axis=1)\n",
    "            data.append(df1)\n",
    "\n",
    "        # else 1 file ==> like before\n",
    "        else:\n",
    "            # read in csv\n",
    "            df = pd.read_csv(names[i], header=None, names=cols)\n",
    "            # remove metadata junk\n",
    "            idx = df.index[df['date'] == 'Date/Time'][0]\n",
    "            df = df.iloc[(idx+1):, :]\n",
    "            df = df.reset_index(drop=True)\n",
    "            # fix column formats\n",
    "            df.value = df.value.astype(float)\n",
    "            # store\n",
    "            data.append(df)\n",
    "\n",
    "        # save name\n",
    "        prev = cur\n",
    "\n",
    "    # fix emptys   \n",
    "    shit = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i].empty:\n",
    "            x_vals = np.arange((i)*len(data[i+1]), (i+1)*len(data[i+1]))\n",
    "            y_vals = np.zeros(x_vals.shape)\n",
    "            y_vals[y_vals == 0] = np.NAN\n",
    "        else:\n",
    "            x_vals = np.arange(i*len(data[i]), (i+1)*len(data[i]))\n",
    "            y_vals = data[i].value\n",
    "        poop = np.column_stack([x_vals, y_vals])\n",
    "        shit.append(poop)\n",
    "    shit = np.concatenate(shit)\n",
    "    plots.append(shit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(len(plots)):\n",
    "    plt.plot(plots[i][:, 0], plots[i][:, 1], label=combs[i], alpha=0.3, linewidth=0.9)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature [C]')\n",
    "plt.savefig('/Users/Farah/Desktop/poop.eps', format='eps', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
